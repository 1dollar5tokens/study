We have 5 storage options
    persistent storage  
    google cloud storage buckets
    google cloud spanner
    google cloud sql
    google cloud bigtable

PERSISTENT STORAGE
    Persistent storage consist of persistent disks that are durable network storage devices that are mapped to your instance. like virtual hard drives (ssd hdd). They can be resized for more storage

    they are independent of instances(vm)
        data stays on the disk even if you delete the vm it will stay on the disk unless you delete the disk also.
    The data is redundant and optimized for performance
    
    Compute engine is the service the supplies the vm with os
        local and durable storage options
        predefined machine types with specific amounts of vCPU and memory
        custom machine types the allow users to tailor their infrastructure to their workload
        integration with google cloud tech. such as Cloud Storage, App Engine, and Big Query

    Vm's is a computer within a computer that allows users to run diff OS and software on a single physical computer. 
        memory
        network
        CPU
        a hypervisor that manages the resources to ensure that each VM operates efficiently
    
    The compute engine, by default, gets deployed with a disk and contains a operating system(os) 
    You can add more disks if you need them
    Disks are can only exist in only one zone in a region and doesnt expand to multiple zones or   regions

    The vm writes data to the disks 3 times in parallel  to achieve a high rate of redundancy
        consider one persistent disk per partition instead of having multiple partitions on a single persistent disk.  For additional partitions, add additional persistent disks

    IOPS - input/output per second

    if the vm needs high IOPS then consider SSD (solid state drive)

    You can add addition disks during the creation of the vm through click ops

    You can take snapshots of the disk even if the vm is running
    
    all data is encrypted by the compute engine before it travels outside the vm and is written on the persistent disk.
        each persistent disk is encrypted either by the system defined or customer supplied keys.  Once you delete a persistent disk, Google discards its cipher keys and that data is irretrievable.
    max size of disk 64tb
    most vm's are max 64tb of disk storage space and max of 16 attached disks.
    custom vm's with less than 3.75gb of memory are limited to 3tb of total persistent disk space and a max of 4 attached disks.
    remember that the total persistent disk space of vm's includes te size of the root disk.

            attaching more than 16 disks is available in the beta feature where you can go up to 128 attached persistent disks for perdefined machine types

    another option in persistent disks is the aility to use LOCAL SSDs that are physically attached to the server that hosts the vm.  It gives you disks with higher throughput and lower latency than the standard or ssd(network based) persistent disks
    LOCAL SSDs connect through both SCSI(small computer system interface(plugs)) AND NVMe(non-volatile memory express interface(connects ssd to CPUs or servers using the PCI express(PCIe) bus)) interfaces, using a LOCAL SSD also does not count towards your network egress bandwidth cap

    The catch is that it only last as long as the vm is up and running.  
    Data remains on the local ssd if you reboot the vm or if you live migrate the vm.
        During a live migration, all data from the local ssd is copied to the vm itself and you will notice a performance decrease.  If the instance is stopped or deleted, all data is lost

    Local SSDs need to be manually stripped into a single logical volume to achieve the best performance.
    Each local SSD disk is 375gb in size and you can attach up to 8 local SSD disks to give you a total of 3tb of storage space per vm


GOOGLE CLOUD STORAGE BUCKET

    For apps that do not have low latency requirements(like first person shooters), GCS Buckets are the answer. Buckets form and object(pics files etc.) storage system that offers a very flexible, reliable, and scalable storage option for a compute engine vm.  Buckets also make it possible to share vm data across multiple vms and zones.  You can use buckets for your on premise vms as well as other cloud service providers.

    The performance is depends on the location of the bucket relative to the vm and the storage class of the bucket you are using.
        There are 4 classes. 
            multi-regional
            regional
            nearline
            coldline
        While all storage classes offer the same throughput and low latency, they differ primarily in their availability, minimum storage durations and pricing.

            To use a cloud storage bucket in an instance, you need to use the google cloud storage FILESYSTEM IN USERSPACE(FUSE) tool, which is an open source adapter for mounting cloud storage buckets. the mounted bucket behaves as a persistent disk and has a higher latency.

        Multi-regional
            is a geo-redundant storage that is appropriate for frequently accessed data.  This ensures that cloud storage will store the data in at least two separate geographical locations.  Cloud makes sure that as soon as you upload the data that it is stored in two different places. It offers a 99.9% availability SLA(Service Level Agreement). Note that multi-regional storage is in select locations.  cost the most of the 4 classes

        Regional
            this is for storing data in the same region and doesnt offer redundancy.  Having data stored within a region gives you better performance.  Regional storage offers a 99.90 SLA.

        Nearline
            This for data that will be rarely used, once a month or so. 99.00 SLA and carries a data retrieval costs along with a 30-day minimus storage duration.

        Coldline
            Is for very rarely accessed data such as archives and archival backups. lowest cost. 99.0 SLA availability and a data retrieval costs along with a 90-day minimum storage duration.

                If no class is selected then it defaults to Multi-regional if in a multi-regional region.  If in just a Regional then the standard with be regional

        Creating a bucket 
            go to storage, create a bucket, name it with an unique name has to be different than everyone else. choose a class.
            You can transfer data from amazon s3 vm or from any other http/https vm or third party buckets in to google storage buckets. this is ideal for data less than 20TB in size

            to transfer select transfer, select a source, and which bucket you want.

            for buckets with 20plus TB a transfer appliance is recommended that can securely migrate data from one bucket to another. currently the appliance is provided on a per-request basis.  there is a form for it in gcp.  

GOOGLE CLOUD SPANNER

    Cloud spanner databases are replicated synchronously and this is done on a global scale.
    The underlying OS that powers the cloud spanner has the ability to replicate at a byte level.  Cloud Spanner stores the databases as a bunch of files and the filesystem takes care of replicating these files across multiple regions.  Cloud spanner stores all data as rows and replicates these rows on to multiple regions for high availability.

    Spanner uses Paxos-based replication scheme where voting replicas participate in a vote for every write request before committing that request.
    This makes it possible for data to be read from any node of your cloud spanner instance simutlaneously.  It is also important to know that there are three types of replicas.

        read-write replicas
        read-only replicas
        witness replicas

    Witness and read-only replicas come into play in a multi-region configuration.

        Read-write replicas maintain full copies of the your data and support both read and write operations.  These are also the only replicas used in single-region configuration. Single-instance replicas only support reads and maintain a full replicated copy for your data from the read-write replicas. Witness replicas do not maintain a full copy of your data and participate in voting to commit writes.

            for learning about votes Http://cloud.google.com/spanner/docs/instances  

        With spanner each node can provide up to 10,000 queries per second(qps) of reads and 2000(qps) of writes.  These, however, can vary depending on your workload and schema design.

        Deploying Spanner

            select spanner on GCP
            instance name firstcloudspanner1
            instance id  firstcloudspanner1
            pick region or multiregion
            # of nodes
            it tells you the cost
            click create

            next database

            click create database
            i can add a table and add columns to my database or use and (SQL)Structured query language
            Sliding Edit as text lets you write SQLstatements to build my database schema

            create a table

                slide to edit as text(optional)
                name the tabel
                add columns :like name age password id
                set primary key 
                    single column or composite
                    chose the column as the primary key ex: id
                create

GOOGLE CLOUD SQL(structured query language)

    SQL is a fully managed relational database service for PostgreSQL and MySQL in the cloud.  Cloud SQL offers many features for both MYsql and Postgresql databases. with cloud sql you can deploy large instances of your Mysql databases.  These vms support mysql 5.6 or 5.7 and up to 416gb of ram and 10tb of storage space. Google also automatically encrypts all customer data on these database vms.  The encrption also extends to Google's internal network and temp files.  Cloud sql also affers data replication across multiple zones and vms.  

    Cloud sql offers similar features for its PostgreSQL(open-source relational database management system(RDBMS)) Postgre version 9.6 is supported with machines of up to 416gb or ram, 32 cpu, and 10tb of storage space, with room to grow in the future.

    When a cloud SQL vm is deployed, Google charges you for the costs of the vms, storage, and network usage. Also for PostgreSQL database, there is a cpu and meory pricing for dedicated core vms


        select SQL create a instance
        choose a database engine and click next to pick from the two types of mysql vms
        its pretty straight forward after that.  There is a configuration button for more options
        click create

    i can connect to the vm using cloudshell

    gcloud sql connect whatevertheinstanceidis --user=root (instance id is the one i created)
    i will be prompted for a password once i run the command

GOOGLE CLOUD BIGTABLE

    Cloud Bigtable is a petabyte(1024TB) scale NoSQL(not only SQL or non-SQL) big data database service that can scale to bittions of rows and thousancs of columns.  Ideal for apps that require high throughput Cloud Bigtable is a perfect storage engine for batch MapReduce operations and machine learning apps. Cloud Bigtable stores data in tables, each of which is sorted key/value map.  The service is also equipped with multiple client libraries that allow you to easily integrate into existing environments.

    Web indexing, google earth and youtube use bigtable

    select bigtable and click create instance
        instance is a container for my cluster

    Enter the name and pick between production(three nodes min.)
    or development instances. click creat when done.

    now i can use Google Cloud Dataproc to use my Bigtable instance
    Dataproc is Google's fully managed service for running Apache Hadoop and Apache Spark







            





    





    